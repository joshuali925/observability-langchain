{"id":"1","clusterStateId":"none","question":"What is an index?","expectedAnswer": "An index is a collection of documents that have similar characteristics. For example, you can have an index for customer data, another index for a product catalog, and yet another index for order data. An index is identified by a name (that must be all lowercase) and contains a mapping that defines multiple fields within the documents. It also contains settings like the number of shards and replicas."}
{"question": "I want to split an index, how to do it?", "expectedAnswer": "You can use the split index api. Here is an example\n<code>\nPUT /sample-index1/_split/split-index1\n{\n  \"settings\": {\n    \"index\": {\n      \"number_of_shards\": 4,\n      \"number_of_replicas\": 2\n    }\n  },\n  \"aliases\": {\n    \"sample-alias1\": {}\n  }\n}\n</code>\nThis will create a new index called \"my_index_2\" with the specified settings and mappings. \nNote that using the Split API can be resource-intensive, particularly for large indices, and may impact the performance of your Opensearch cluster. You should consider factors such as shard size and hardware resources when planning to split an index using the Split API."}
{"question": "I call split index api, it give me this response. Is it succeed?\n{\n    \"acknowledged\": true,\n    \"shards_acknowledged\": false,\n    \"index\": \"split-index1\"\n}", "expectedAnswer": "No, the API has not been executed successfully at this time. \"acknowledged\" is true means opensearch server acknowledge the request, but \"shards_acknowledged\" is false means not all shards acknowledge are started now. This can happen if there are not enough resources available in your Opensearch cluster to create the desired number of shards or if the shards are taking too long to initialize, or not enough disk space.\nThe split process can be monitored with cat recovery api\n<code>\nGET /_cat/recovery/<target>\nGET /_cat/recovery\n</code>"}
{"question": "I have an index for search, now the search requests are increasing, I want to increase the shard numbers. How to do it?", "expectedAnswer": "Users can not change the shard number of existing index directly. You can create a new index with more shards.\nYou can use the split index api or reindex api to achieve this."}
{"question": "What's the difference of split index api and reindex api?", "expectedAnswer": "Both of them can be used to increase the number of shards for an index, but they differ in their approach.\nWhen you call split index api, Opensearch creates a new index with the desired number of shards and copies the data from the original index to the new index. But the number of shards must be a factor of 2. The number of times the index can be split (and the number of shards that each original shard can be split into) is determined by the index.number_of_routing_shards setting. The target shard numbers is a multiple of the original shard numbers and is a factor of index.number_of_routing_shards. \nOn the other hand, the reindex API is used to copy data from one index to another index. This approach is more flexible as you can specify any valid target index shard count and modify field names or use a query to filter documents. But this approach is more computationally expensive in terms of resource consumption."}
{"question": "I ran into a error whose content is \"index template [2] has index patterns [1] matching patterns from existing templates [1] with patterns (1 => [1]) that have the same priority [0], multiple index templates may not match during index creation, please use a different priority\", could you tell me how to work it out?", "expectedAnswer": "The error message you received in OpenSearch indicates that there are multiple index templates with conflicting index patterns and priorities. The error suggests that you use a different priority for one of the templates to resolve this conflict.\nHere are the steps you can take to resolve this issue:\nIdentify the conflicting templates: The error message indicates that there are two templates with conflicting index patterns. You need to identify these templates to fix the issue.\nDecide which template needs to be modified: Once you have identified the conflicting templates, you need to decide which template needs to be modified. You can do this by checking the index patterns and priorities for each template.\nModify the priority of one of the templates: To resolve the conflict, you need to modify the priority of one of the templates. You can do this by updating the priority field in the template's JSON file.\nFor example, if you have two templates with the same priority, you can modify one of the templates to have a higher or lower priority, depending on your needs. You can set the priority to a positive or negative integer value.\nApply the modified template: After modifying the template, you need to apply the changes by uploading the modified JSON file to OpenSearch or using the API.\nVerify the changes: Finally, you should verify that the changes have been applied correctly by creating an index using the modified template.\nBy following these steps, you should be able to resolve the \"index template has index patterns matching patterns from existing templates\" error in OpenSearch."}
{"question": "How can I get the free disk space information in opensearch?", "expectedAnswer": "curl -X GET \"localhost:9200/_cat/allocation?h=disk.used_percent,disk.total,disk.used,disk.free,node.role&bytes=b&format=json\"\nThis command will return the following fields:\ndisk.used_percent: Percentage of disk space used.\ndisk.total: Total disk space in bytes.\ndisk.used: Used disk space in bytes.\ndisk.free: Free disk space in bytes.\nnode.role: Whether the node is a master, data or client node."}
{"question": "I'm using opensearch v1.2, it's cluster health status is red, do you know why?", "expectedAnswer": "Unassigned Shards: One of the most common reasons for a red cluster status is unassigned shards. If a shard is not assigned to any node in the cluster, it becomes unassigned. This can happen due to various reasons such as node failure, network partition, or a misconfiguration in the cluster. To check if this is the case, you can run the command GET _cluster/allocation/explain?pretty and look for unassigned shards.\nInsufficient Disk Space: If a node in the cluster runs out of disk space, it can cause the cluster health status to turn red. This can happen if you have not allocated enough disk space for your indices or if you have a large amount of data being indexed. To check if this is the case, you can run the command GET _cat/nodes?v&h=dup and check the disk usage percentage column.\nNode Failure: If one or more nodes in the cluster fail, the remaining nodes will have to handle the extra workload, and the cluster health may turn red. Node failure can occur due to hardware failure, software issues, or network problems. To check if this is the case, you can run the command GET _cat/nodes?v and look for any nodes that are down.\nUnresponsive Nodes: If one or more nodes in the cluster are unresponsive, the remaining nodes will have to handle the extra workload, and the cluster health may turn red. This can happen due to network issues or hardware problems. To check if this is the case, you can run the command GET _cat/nodes?v and look for any nodes that are marked as disconnected or dead."}
{"question": "{\"Message\":\"User: anonymous is not authorized to perform: es:ESHttpGet with an explicit deny in a resource-based policy\"}", "expectedAnswer": "The error message indicates that the user \"anonymous\" is not authorized to perform an HTTP GET request on a resource in Amazon OpenSearch Service due to an explicit deny in a resource-based policy.\nTo resolve this issue, You can log in to AWS OpenSearch Admin console, click on the \u201cSecurity configuration\u201d tab, you should see the \"Access policies\" tab along with other security-related options.\nOnce you have identified the policy, you can modify it to grant the necessary permissions to the \"anonymous\" user or create a new policy that allows the user to perform the required actions. "}
{"question": "Why opensearch node CPU is high?", "expectedAnswer": "There can be several reasons why an OpenSearch node's CPU usage is high, such as:\nHeavy indexing or search load: If the node is handling a large number of indexing or search requests, it may lead to high CPU usage.\nLarge queries: If the node is executing a large query, it may cause high CPU usage, especially if the query requires a lot of computation or data processing.\nGarbage collection: If the node is spending a lot of time on garbage collection, it may lead to high CPU usage. This can happen if the heap size is not optimized for the workload."}
{"question": "how to identify large queries?", "expectedAnswer": "You can use the Slow Log feature. The Slow Log captures queries that take longer than a certain threshold time to execute, allowing you to identify the queries that may be causing performance issues.\nYou can enable the Slow Log in OpenSearch by modifying the elasticsearch.yml file or using the Cluster Update Settings API. \nTo specifically identify large queries, you can sort the Slow Log entries by the query_duration field and look for entries with a high value. "}
{"question": "How can I tell if there is Heavy indexing or search load?", "expectedAnswer": "Use the OpenSearch Performance Analyzer: The Performance Analyzer is a tool that provides real-time metrics and monitoring data for OpenSearch nodes, indices, and shards. You can use it to identify CPU, memory, and disk usage, indexing and search rates, and other performance metrics. You can access the Performance Analyzer through the OpenSearch REST API.\nCheck the indexing and search rate: You can check the indexing and search rate using the Indexing and Search Stats APIs. If the indexing or search rate is high, it may indicate heavy indexing or search load.\nCheck the query response time: If the query response time is slow, it may indicate heavy search load. You can use the Search Profiler API to analyze the query and identify any bottlenecks."}
{"question": "What are dangling indices in OpenSearch?", "expectedAnswer": "Dangling indices mean the indices whose data is absent from the current cluster state, in other words, the indices whose data exist but metadata do not exist are considered to be dangling indices."}
{"question": "How do dangling indices come about in OpenSearch cluster?", "expectedAnswer": "There are many reasons which can cause dangling indices to be produced in OpenSearch cluster, but typically this happens when you delete more than cluster.indices.tombstones.size indices after one or more nodes of your OpenSearch cluster are offline. The cluster level setting cluster.indices.tombstones.size is a static setting which is used to prevent nodes rejoined the cluster from reimporting the indices which are deleted before, this setting defaults to 500, so if you delete more than 500 indices when a node is offline, and the node has shards of each index, after the node is back, indices exceeding 500 will be reimported into the cluster, these indices are called dangling indices."}
{"question": "How can I handle the situation when dangling indices exist in OpenSearch cluster?", "expectedAnswer": "You can use dangling indices API to handle this situation. Firstly, you can use list dangling indices API(GET _dangling) to list all of the dangling indices; secondly, you can use import dangling indices API(POST _dangling/<index-uuid>?accept_data_loss=true) to import one dangling index into the cluster, or use delete dangling indices API(DELETE _dangling/<index-uuid>?accept_data_loss=true) to delete the dangling index specified."}
{"question": "Is there an UI for reindex?", "expectedAnswer": "Yes, Opensearch Dashboard introduce index management UI enhancements to Simplify cluster operations in version 2.5."}
{"question": "Where can I find it in the dashboard UI?", "expectedAnswer": "The UI enhancements of cluster operations is implemented in index management dashboards plugin, and the UI components is in the directory of Opensearch Plugins -> Index Management -> indices."}
{"question": "Which operations are introduced in 2.5?", "expectedAnswer": "The opensearch 2.5 UI enhancements provide a more user-friendly interface to run common indexing and data stream operations. Now you can perform create, read, update, and delete (CRUD) and mapping for indexes, index templates, and aliases through the UI as well as open, close, reindex, shrink, and split indexes."}
{"question": "I want to call performance analyzer api to get metrics, but get an error below:\nFailed to connect to localhost port 9600 after 0 ms: Connection refused", "expectedAnswer": "The error message \"Failed to connect to localhost port 9600 after 0 ms: Connection refused\" suggests that the performance analyzer RCA agent is not running or is not listening on port 9600 on your local machine.\nTo manually start performance analyzer RCA agent, you can use below command:\n<code>\nOPENSEARCH_HOME=/path/to/opensearch OPENSEARCH_JAVA_HOME=/path/to/opensearch/jdk OPENSEARCH_PATH_CONF=/path/to/opensearch/bin ./performance-analyzer-agent-cli\n</code>\nTo configure the port of performance analyzer RCA agent, you will need to edit the performance-analyzer.properties configuration file in the config/opensearch-performance-analyzer/ directory. The property is webservice-listener-port."}
{"question": "Using curl on the command line is a bit cumbersome. Is there a way to use OpenSearch Dashboards Dev Tools to call the PA API?", "expectedAnswer": "Sure, you can use dev tools to make a request on port 9200 to call the performance analyzer api, but you need to add \u201c_agent\u201d to your url path. Here is a sample request in dev tools:\n<code>\nGET _plugins/_performanceanalyzer/_agent/metrics?metrics=Latency,CPU_Utilization&agg=avg,max&dim=ShardID\n</code>"}
{"question": "What does the parameter max_shard_size of Shrink API mean in OpenSearch?", "expectedAnswer": "The parameter max_shard_size of Shrink API is used to specify the maximum size of a primary shard in the target index, OpenSearch use this parameter and the total storage for all primary shards in the source index to calculate the optimal number of primary shards for the target index. For example, if an index has 10 primary shards and occupy a total of 100GB of storage, if max_shard_size is set to 50GB, then the number of primary shards in the target index will be 2. "}
{"question": "Can I set the parameter max_shard_size and index.number_of_shards at the same time when I call the Shrink API in OpenSearch?", "expectedAnswer": "No, you can\u2019t, these two parameters are conflicting, you can only use one of them."}
{"question": "What\u2019s you suggestion for the value of the parameter max_shard_size of Shrink API in OpenSearch?", "expectedAnswer": "The optimal value of the parameter max_shard_size is 30GB, no more than 50GB, that means the storage of each shard in the target index will not exceed 30GB or 50GB, which will improve the performance and stability of you OpenSearch cluster."}
{"question": "Can you recommend some tools which can be used to migrate indices between different OpenSearch clusters?", "expectedAnswer": "There are many tools can be used to migrate indices between different OpenSearch clusters, you can see below:\n1. Reindex, which is inside OpenSearch and can be used directly, no other server and cost are needed.\n2. elastic-dump, which is an open-source tool, written by Python and can be used to migrate indices between clusters.\n3. Logstash, which is common-used tool, written by Java&Ruby and can be used to migrate indices between clusters.\n4. Snapshot, you can also use Snapshot APIs to migrate indices between clusters, take a snapshot in the source cluster, and then restore the snapshot in the destination cluster.\n5. CCR(cross-cluster replication), the cross-cluster replication plugin of OpenSearch lets you replicate indices from one cluster to another, you can use this feature to migrate indices between clusters."}
{"question": "The indices in the source cluster occupy hundreds of MB of storage, which tool do you recommend to migrate these indices to another cluster?", "expectedAnswer": "I suggest you use Reindex, which can be used directly by calling the Reindex API, you do not need to do extra work and buy a server to migrate your indices."}
{"question": "The indices in the source cluster occupy hundreds of GB of storage, which tool do you recommend to migrate these indices to another cluster?", "expectedAnswer": "I suggest you use Snapshot or CCR, when you want total migration for the indices which occupy hundreds of GB of storage, you can use Snapshot APIs to do the migration, but when you want incremental migration for the indices, you can use CCR, that means the write operations on the source indices during the migration will also be replicated to the target indices in the destination clusters."}
{"question": "I'm using opensearch and a node is down. how to find reasons?", "expectedAnswer": "There could be several reasons why a cluster is down. Here are some methods you can take to diagnose the problem:\n1.Check the opensearch logs.\n2.Check the system logs\n3.Use the performance analyzer and root cause analysis api.\n4.Check the network connectivity.\n5.Check the hardware.\n"}
{"question": "What are the available RCA APIs that can be used?", "expectedAnswer": "You can use this api to get all running RCA:\n<code>\nGET _plugins/_performanceanalyzer/_agent/rca?all\n</code>\nHowever, only cluster RCA and temperature RCA can be requested using api. Here are all available cluster RCA: \"QueueRejectionClusterRca\", \"ClusterTemperatureRca\", \"HotShardClusterRca\", \"ShardRequestCacheClusterRca\", \"FieldDataCacheClusterRca\", \"HotNodeClusterRca\", \"HighHeapUsageClusterRca\". Cluster RCA can only be requested on cluster manager node."}
{"question": "How to get the shard consuming most cpu resources when the cluster down?", "expectedAnswer": "You can use performance analyzer api to do this. Here is the api:\n<code>\nGET _plugins/_performanceanalyzer/_agent/metrics?metrics=CPU_Utilization&agg=max&dim=ShardID\n</code>"}
{"question": "what's reindex api in opensearch", "expectedAnswer": "Reindex API is a REST API that copies documents from any existing index, alias, or data stream to another one. It can also reindex documents from a remote cluster. You can use Reindex API to change the mappings, settings, or aliases of your indices, or to upgrade your opensearch version."}
{"question": "How do I configure remote reindexing in OpenSearch?", "expectedAnswer": "To configure remote reindexing in OpenSearch, you need to make sure that the remote hosst is accessible from the current host.\nFor example:\n<code>\nPOST _reindex { \"source\": { \"remote\": { \"host\": \"https://<provided_endpoint>:443\",\"username\": \"xxx\", password: \"xxxx\" }, \"index\": \"source_index\" }, \"dest\": { \"index\": \"target_index\" } }\n</code>"}
{"question": "How do I monitor the progress of reindexing?", "expectedAnswer": "To monitor the progress of reindexing in OpenSearch, you can use the tasks API to get information about the status and duration of your reindexing task. You can also use the wait_for_completion parameter to make your Reindex API request synchronous and return only when the task is complete.\nFor example, if you want to monitor a reindexing task with ID 1234567890 on your target host, you can send this request:\n\n<code>\nGET _tasks/1234567890\n</code>\n\nThis will return a response with details such as:\n\nThe node where the task is running\nThe start time and running time of the task\nThe number of documents created, updated, deleted, or failed\nThe status of subtasks such as bulk requests\nAny errors or failures that occurred during the task\nYou can also cancel a running reindexing task by sending this request:\n<code>\nPOST _tasks/1234567890/_cancel\n</code>"}
{"question": "where can i find opensearch logs", "expectedAnswer": "On Docker and tarball, the location is openserach/logs. On most Linux installations, OpenSearch writes logs to /var/log/opensearch/"}
{"question": "how can i change log level to debug", "expectedAnswer": "There are other ways to do this. \n1. You can change the log level to debug by adding the following line to opensearch.yml: logger.org.opensearch.index.reindex: debug\n2. You can also modify log4j2.properties: logger.reindex.name = org.opensearch.index.reindex and logger.reindex.level = debug2.\n3. \n<code>\nPUT /_cluster/settings\n{\n    \"persistent\" : {\n        \"logger.org.opensearch.index.reindex\" : \"DEBUG\"\n    }\n}\n</code>"}
{"question": "how to enable slow query log", "expectedAnswer": "To enable slow query log, you can do this\n<code>\nPUT /index/_settings\n{\n    \"index.search.slowlog.threshold.query.warn\": \"5s\",\n    \"index.search.slowlog.threshold.query.info\": \"2s\"\n}\n</code>"}
{"question": "Query on my opensearch cluster is slow, how can i find most time consuming task", "expectedAnswer": "To find the most time-consuming task on your opensearch cluster, you can use the hot_threads API. This API returns a list of threads that are consuming the most CPU time. You can use this information to identify the most time-consuming tasks and optimize them.\nHere's an example of how to use the hot_threads API:\n<code>\ncurl -XGET 'http://localhost:9200/_nodes/hot_threads'\n</code>\nThis will return a list of threads that are consuming the most CPU time. You can then use this information to optimize your cluster."}
{"question": "Opensearch shard is not allocated due to node_decision:throttled, reached the limit of incoming shard recoveries", "expectedAnswer": "When an Opensearch node leaves the cluster, it triggers a shard recovery process for the shards that were allocated to that node. During the recovery process, the shards are copied from other nodes to the remaining nodes in the cluster. If there are too many shard recoveries happening at once, it can cause a bottleneck in the system, leading to performance issues and potential data loss.\nTo prevent this from happening, Opensearch has a limit on the number of concurrent shard recoveries that can happen at once. If the number of incoming shard recoveries exceeds this limit, new recoveries will be throttled, and the cluster will start to report that it has reached the limit of incoming shard recoveries. This is a mechanism to prevent overload and ensure that the cluster can continue to function smoothly."}
{"question": "will shard being throttled get lost eventually?", "expectedAnswer": "Shards being throttled will not get lost eventually. Throttling of shards typically occurs when the system is under heavy load or when there are resource constraints. When a shard is throttled, it means that the allocation of that shard to a node has been delayed to ensure that the node is not overloaded with too many shard recoveries at once.\nOnce the system load decreases or the resource constraints are resolved, the shard allocation process will resume, and the throttled shards will be allocated to nodes as soon as possible. Therefore, the throttled shards will not be lost, but it may take some time for them to be fully recovered and become available for search and other operations."}
{"question": "now throttling indexing error in opensearch server", "expectedAnswer": "it means that the rate at which data is being indexed is exceeding the capacity of the system to handle it. This can happen if the index is not optimized for indexing, the hardware or resources are not sufficient, or the indexing rate is too high.\nTo address throttling during indexing, you can take the following steps:\nOptimize the index settings: Make sure that the index is optimized for efficient indexing by adjusting the index settings such as the number of shards, replicas, refresh and flush intervals, and bulk size.\nUpgrade hardware: If the system is struggling to handle the indexing load, you may need to upgrade the hardware or add more resources such as memory, disk space, or CPU.\nReduce indexing rate: If the indexing rate is too high, you can reduce it to a more manageable level by adjusting the index settings, slowing down the ingestion rate, or using techniques such as throttling or batching.\nMonitor system metrics: Keep an eye on the system metrics such as CPU usage, memory usage, disk I/O, and network traffic to identify any bottlenecks or performance issues that may be contributing to the throttling.\nReview logs: Check the OpenSearch logs for any errors, warnings, or exceptions related to indexing, and take corrective actions as necessary."}
{"question": "I need to set up a search service by using OpenSearch and the qps will be 100, data storage will be 1TB per day. What kind of machine configuration should I consider?", "expectedAnswer": "The machine configuration required for running OpenSearch and supporting 1TB of data storage per day with a 100 QPS workload will depend on various factors such as the size of the documents, complexity of queries, indexing frequency, and size of the cluster.\nHere are some general guidelines to consider:\nCPU: OpenSearch is a resource-intensive application, and you will need a powerful CPU to handle the workload. Consider using processors with multiple cores to distribute the load efficiently.\nMemory: OpenSearch is also memory-intensive, and you will need enough RAM to support the size of your data and the queries you will be running. You should aim for at least 64GB or more of RAM for good performance.\nStorage: To handle 1TB of data per day, you will need fast and reliable storage. Consider using SSDs or NVMe drives for optimal performance.\nNetworking: You will need a high-speed network connection to handle the 100 QPS workload. Consider using 10GbE or faster network interfaces.\nCluster size: The number of nodes required in your cluster will depend on your data size, query complexity, and indexing frequency. You may need to start with a small cluster and scale up as needed.\nBased on these guidelines, you may want to consider a machine configuration with at least 64GB of RAM, multiple CPU cores, and fast SSDs or NVMe drives. You can also consider using a cloud-based solution such as Amazon Web Services or Microsoft Azure to manage your OpenSearch cluster and scale resources up or down as needed."}
{"question": "How many nodes should I set?", "expectedAnswer": "The number of nodes required for your OpenSearch cluster will depend on several factors, including your data size, query complexity, indexing frequency, and expected workload. It is difficult to give an exact number without knowing more about your specific requirements, but I can provide some general guidelines that can help you determine the number of nodes required.\nOpenSearch is designed to be horizontally scalable, which means that you can add more nodes to the cluster to handle an increasing workload or larger data size. Generally, it is recommended to start with a small cluster and scale up as needed. A minimum of three nodes is recommended for redundancy and availability purposes.\nAs a starting point, you could consider setting up a three-node cluster, with one node acting as the master node, and the other two nodes as data nodes. If you find that the workload or data size increases beyond what the cluster can handle, you can add more nodes to the cluster to distribute the load and improve performance.\nKeep in mind that the number of nodes you need will also depend on your data and query patterns. For example, if your queries are computationally intensive or require a lot of memory, you may need more nodes to handle the workload.\nIn summary, the number of nodes required for your OpenSearch cluster will depend on several factors, including your data size, query complexity, and expected workload. Starting with a three-node cluster and scaling up as needed is a good starting point, but you may need to adjust the number of nodes based on your specific requirements."}
{"question": "{\"category\":[\"Men's Clothing\"],\"currency\":\"EUR\",\"customer_first_name\":\"Robert\",\"customer_full_name\":\"Robert Mcdonald\",\"customer_gender\":\"MALE\",\"customer_id\":29,\"customer_last_name\":\"Mcdonald\",\"customer_phone\":\"\",\"day_of_week\":\"Saturday\",\"day_of_week_i\":5,\"email\":\"robert@mcdonald-family.zzz\",\"manufacturer\":[\"Elitelligence\",\"Low Tide Media\"],\"order_date\":\"2023-02-11T04:50:53+00:00\",\"order_id\":562853,\"products\":[{\"base_price\":10.99,\"discount_percentage\":0,\"quantity\":1,\"manufacturer\":\"Elitelligence\",\"tax_amount\":0,\"product_id\":21053,\"category\":\"Men's Clothing\",\"sku\":\"ZO0564705647\",\"taxless_price\":10.99,\"unit_discount_amount\":0,\"min_price\":5.39,\"_id\":\"sold_product_562853_21053\",\"discount_amount\":0,\"created_on\":\"2016-12-10T04:50:53+00:00\",\"product_name\":\"Print T-shirt - white/blue\",\"price\":10.99,\"taxful_price\":10.99,\"base_unit_price\":10.99},{\"base_price\":7.99,\"discount_percentage\":0,\"quantity\":1,\"manufacturer\":\"Low Tide Media\",\"tax_amount\":0,\"product_id\":23834,\"category\":\"Men's Clothing\",\"sku\":\"ZO0481004810\",\"taxless_price\":7.99,\"unit_discount_amount\":0,\"min_price\":4.07,\"_id\":\"sold_product_562853_23834\",\"discount_amount\":0,\"created_on\":\"2016-12-10T04:50:53+00:00\",\"product_name\":\"3 PACK - Socks - blue/grey\",\"price\":7.99,\"taxful_price\":7.99,\"base_unit_price\":7.99}],\"sku\":[\"ZO0564705647\",\"ZO0481004810\"],\"taxful_total_price\":18.98,\"taxless_total_price\":18.98,\"total_quantity\":2,\"total_unique_products\":2,\"type\":\"order\",\"user\":\"robert\",\"geoip\":{\"country_iso_code\":\"SA\",\"location\":{\"lon\":45,\"lat\":25},\"continent_name\":\"Asia\"},\"event\":{\"dataset\":\"sample_ecommerce\"}} I want to import this doc into OpenSearch, could you give me the proper index mappings of it?", "expectedAnswer": "Sure, I can suggest an index mapping that you can use for this data. Here's an example mapping that you can use for the sample_ecommerce index:\n<code>\nPUT /sample_ecommerce\n{\n\"mappings\": {\n\"properties\": {\n\"category\": {\n\"type\": \"keyword\"\n},\n\"currency\": {\n\"type\": \"keyword\"\n},\n\"customer_first_name\": {\n\"type\": \"text\"\n},\n\"customer_full_name\": {\n\"type\": \"text\"\n},\n\"customer_gender\": {\n\"type\": \"keyword\"\n},\n\"customer_id\": {\n\"type\": \"integer\"\n},\n\"customer_last_name\": {\n\"type\": \"text\"\n},\n\"customer_phone\": {\n\"type\": \"keyword\"\n},\n\"day_of_week\": {\n\"type\": \"keyword\"\n},\n\"day_of_week_i\": {\n\"type\": \"integer\"\n},\n\"email\": {\n\"type\": \"keyword\"\n},\n\"manufacturer\": {\n\"type\": \"keyword\"\n},\n\"order_date\": {\n\"type\": \"date\"\n},\n\"order_id\": {\n\"type\": \"integer\"\n},\n\"products\": {\n\"type\": \"nested\",\n\"properties\": {\n\"base_price\": {\n\"type\": \"float\"\n},\n\"discount_percentage\": {\n\"type\": \"float\"\n},\n\"quantity\": {\n\"type\": \"integer\"\n},\n\"manufacturer\": {\n\"type\": \"keyword\"\n},\n\"tax_amount\": {\n\"type\": \"float\"\n},\n\"product_id\": {\n\"type\": \"integer\"\n},\n\"category\": {\n\"type\": \"keyword\"\n},\n\"sku\": {\n\"type\": \"keyword\"\n},\n\"taxless_price\": {\n\"type\": \"float\"\n},\n\"unit_discount_amount\": {\n\"type\": \"float\"\n},\n\"min_price\": {\n\"type\": \"float\"\n},\n\"_id\": {\n\"type\": \"keyword\"\n},\n\"discount_amount\": {\n\"type\": \"float\"\n},\n\"created_on\": {\n\"type\": \"date\"\n},\n\"product_name\": {\n\"type\": \"text\"\n},\n\"price\": {\n\"type\": \"float\"\n},\n\"taxful_price\": {\n\"type\": \"float\"\n},\n\"base_unit_price\": {\n\"type\": \"float\"\n}\n}\n},\n\"sku\": {\n\"type\": \"keyword\"\n},\n\"taxful_total_price\": {\n\"type\": \"float\"\n},\n\"taxless_total_price\": {\n\"type\": \"float\"\n},\n\"total_quantity\": {\n\"type\": \"integer\"\n},\n\"total_unique_products\": {\n\"type\": \"integer\"\n},\n\"type\": {\n\"type\": \"keyword\"\n},\n\"user\": {\n\"type\": \"keyword\"\n},\n\"geoip\": {\n\"properties\": {\n\"country_iso_code\": {\n\"type\": \"keyword\"\n},\n\"location\": {\n\"type\": \"geo_point\"\n},\n\"continent_name\": {\n\"type\": \"keyword\"\n}\n}\n},\n\"event\": {\n\"properties\": {\n\"dataset\": {\n\"type\": \"keyword\"\n}\n}\n}\n}\n}\n}\n\n</code>"}
{"question": "What is the cluster manager task throttling in OpenSearch?", "expectedAnswer": "Cluster manager task throttling limits is a feature introduced in OpenSearch 2.4, this feature lets you set limits on the volume of tasks such as create-index, put-mapping, and update-settings submitted by nodes to the cluster manager. By throttling the incoming tasks submitted to the cluster manager, you can avoid scenarios in which spikes in the number of pending tasks  can affect the cluster manager\u2019s performances, which will result in improving the cluster\u2019s availability and resiliency."}
{"question": "How can I use the cluster manager task throttling in OpenSearch?", "expectedAnswer": "You can use the cluster manager task throttling feature by calling the update cluster settings API in OpenSearch, you can set throttling limits for each task type in the cluster_manager.throttling.thresholds object, here is an example:\nPUT _cluster/settings\n{\n  \"persistent\": {\n    \"cluster_manager.throttling.thresholds\" : {\n      \"put-mapping\" : {\n          \"value\" : 1000\n      }\n    }\n  }\n}\nyou can see that this example set throttling limits to 1000 for the put-mapping task, if the number of put-mapping tasks exceeds 1000, the cluster manager will reject the incoming put-mapping tasks."}
{"question": "Is the cluster manager task throttling feature enabled by default?", "expectedAnswer": "No, it is not. The cluster manger task throttling feature is disabled by default, you should set throttling limit for the task which you want to throttle explicitly. "}
{"question": "What is the search backpressure in OpenSearch?", "expectedAnswer": "Search backpressure is a feature introduced in OpenSearch 2.4, this feature protects your OpenSearch clusters against traffic surges and other issues that can overwhelm cluster resources and lead to degraded performance or failures. This feature keep monitoring and identifying resource-intensive search requests and cancel them when the node is under duress, so it can keep nodes and clusters stable during periods of high traffic."}
{"question": "How can I use the search backpressure in OpenSearch?", "expectedAnswer": "You can use the search backpressure feature by calling the update cluster settings API in OpenSearch, there are so many settings for search backpressure, here is an example that set search_backpressure.mode to enforced so that OpenSearch can cancel search requests which have breached the resource limits: \nPUT _cluster/settings\n{\n  \"persistent\": {\n    \"search_backpressure.mode\" : \"enforced\"\n  }\n}"}
{"question": "How do I know the search backpressure takes effect in OpenSearch?", "expectedAnswer": "You can use the node stats API to monitor the search request cancellations by the search pressure feature, here is the request example:\nGET _nodes/stats/search_backpressure"}
{"question": "What kind of roles and permissions should I grant for users to use index management plugin of OpenSearch?", "expectedAnswer": "When granting roles and permissions for users to use the index management plugin of OpenSearch, it's important to consider the level of access they need to perform their tasks without compromising security.\nHere are some guidelines for the roles and permissions to grant:\nThe user should have the cluster:admin/indices permission to manage indices.\nIf the user needs to create and delete indices, grant them the indices:admin/create and indices:admin/delete permissions, respectively.\nIf the user needs to modify index settings, such as the number of shards or replicas, grant them the indices:admin/settings/update permission.\nIf the user needs to manage index aliases, grant them the indices:admin/aliases permission.\nIf the user needs to manage index templates, grant them the indices:admin/template permission.\nIf the user needs to perform more advanced index management tasks, such as creating and modifying pipelines, grant them the ingest:admin/pipeline permission.\nIf the user needs to view or manage snapshot and restore operations, grant them the cluster:admin/snapshot permission.\nKeep in mind that these permissions should be granted based on the specific needs and responsibilities of each user, and should be limited to only what is necessary for them to perform their tasks. Additionally, it's important to regularly review and update these permissions to ensure that users have the appropriate level of access and that the system remains secure."}
{"question": "what's relocating shards? Does it indicate a problem?", "expectedAnswer": "In OpenSearch, relocating shards refer to the process of moving a shard from one node to another. This could be due to various reasons such as node failure, rebalancing, or maintenance activities.\nRelocating shards do not necessarily indicate a problem, and it is a normal process in a distributed system like OpenSearch. In fact, OpenSearch automatically rebalances shards across nodes to ensure that the data is evenly distributed, and relocating shards is part of this process.\nHowever, if you notice a high number of relocating shards or the process takes an extended time, it could be an indication of an underlying problem such as node or network performance issues. In such cases, it's important to investigate the root cause of the problem and take necessary actions to resolve it."}
{"question": "There is a high number of relocating shards. What actions can I take?", "expectedAnswer": "If you notice that there are many relocating shards in your OpenSearch cluster, there are a few actions you can take:\n \nCheck the health of the cluster: Make sure that the cluster is healthy and there are no issues with the nodes or the network.\n \nCheck the disk usage on the nodes: If the nodes are running out of disk space, it can cause shards to be relocated. You may need to add more nodes or increase the disk space on the existing nodes.\n \nMonitor the performance of the nodes: If the nodes are under heavy load, it can cause shards to be relocated. You may need to optimize your queries or add more nodes to handle the load.\n \nMonitor the network latency: If the network latency is high, it can cause shards to be relocated. You may need to optimize the network or add more nodes to reduce the load on each node.\n \nCheck the allocation settings: Make sure that the allocation settings are properly configured and optimized for your use case. You can use the allocation explain API to see why shards are being relocated and adjust the settings accordingly.\n \nIn general, relocating shards is not always a problem and can be a normal part of the cluster's operation. However, if you notice that there are a large number of relocating shards or the relocation process is taking a long time, it is a sign that there may be an issue that needs to be addressed."}
{"question": "what is considered as a large number of relocating shards?", "expectedAnswer": "The number of relocating shards considered as \"large\" depends on the total number of shards in the cluster and the overall size of the cluster. However, as a general rule of thumb, if the number of relocating shards is consistently higher than the number of other shard states (such as initializing, started, or unassigned), it may indicate a problem with the cluster's health."}
{"question": "Why there are unassigned shards?", "expectedAnswer": "Unassigned shards in OpenSearch can occur for several reasons, including:\n \nNodes are offline: If a node in the cluster goes down or is otherwise unavailable, the shards that were assigned to it will become unassigned.\n \nIndex settings or mappings have changed: If changes are made to the index settings or mappings, some of the existing shards may become unassigned until the new settings are applied.\n \nInsufficient disk space: If there is not enough disk space available to store new shards, or if the disk space is limited and cannot be expanded, then shards may remain unassigned.\n \nCluster allocation settings: If the cluster allocation settings are set to exclude certain nodes or types of nodes, then shards may become unassigned.\n \nRestore to a closed index\n \nHigh memory pressure\n \nLimitation on max number of shards on a node is reached"}
{"question": "What\u2019s the different between unassigned shards and delayed unassigned shards?", "expectedAnswer": "Delayed unassigned shards are a specific type of unassigned shards in OpenSearch that are waiting to be assigned to a node, but have been delayed due to some reason. This delay can occur because the node that the shard is supposed to be assigned to is not yet available, or because there are some constraints that prevent the shard from being assigned to that node.\n \nOn the other hand, unassigned shards refer to any shard that has not been assigned to a node. This can happen when a node fails, a new node is added, or when an index is created or updated.\n \nTherefore, delayed unassigned shards are a subset of unassigned shards, where the assignment of the shard is delayed due to some reason."}
{"question": "how to tell an unassigned shard is due to delay or not?", "expectedAnswer": "You can check the reason for an unassigned shard by calling the _cat/shards API endpoint in OpenSearch. This will return a list of all shards in the cluster, including information on their state, index, and the node they are assigned to (if any).\n \nIf a shard is unassigned due to a delay, the reason will be listed as \"delayed\", along with the time in milliseconds that the shard has been delayed. If the reason is not \"delayed\", then there may be another issue preventing the shard from being assigned, such as a lack of available nodes or disk space.\n \nYou can also check the cluster's allocation explain API by calling _cluster/allocation/explain to get more information about why a particular shard is unassigned. This API provides a detailed explanation of why a particular shard could not be allocated, including any rules or conditions that prevented allocation."}
{"question": "how to install a plugin for opensearch?", "expectedAnswer": "To install a plugin for OpenSearch, you can follow these general steps:\nDownload the plugin: First, you need to download the plugin you want to install. OpenSearch plugins are typically distributed as ZIP files.\nInstall the plugin: Once you have downloaded the plugin, you can install it by running the following command from the OpenSearch directory:\n<code>\nbin/opensearch-plugin install file:///path/to/plugin.zip\n</code>\nAlternatively, you can install a plugin directly from a URL by running the following command:\n<code>\nbin/opensearch-plugin install https://url/to/plugin.zip\n</code>\nVerify the installation: After installing the plugin, you can verify that it has been successfully installed by running the following command:\n<code>bin/opensearch-plugin list</code>\nThis will list all the installed plugins, including the newly installed one."}
{"question": "can i install a plugin from maven repository?", "expectedAnswer": "Yes, some OpenSearch plugins are available in Maven repositories, and you can install them directly from there.\n<code>\nbin/opensearch-plugin install groupId:artifactId:version\n</code>\nan example \n<code>\nsudo ./opensearch-plugin install org.opensearch.plugin:opensearch-anomaly-detection:2.2.0.0\n</code>\nRestart your OpenSearch node after installing a plugin.\n<url>\nhttps://opensearch.org/docs/2.6/install-and-configure/plugins/\n</url>"}
{"question": "how to tune for disk usage for opensearch?", "expectedAnswer": "There are different ways to tune disk usage for OpenSearch depending on your needs and goals.\n1. Disable the features you don't need, like don't index filed never used for filter\n2. Don't use default dynamic string mappings, teh default dynamic string mappings will index string fields both as text and keyword, this is wasterful if you only need one of them.\n3. Wathch your shard size, Larger shards are going to be more efficient at storing data. To increase the size of your shards, you can decrease the number of primary shards in an index by creating indices with fewer primary shards, creating fewer indices (e.g. by leveraging the Rollover API), or modifying an existing index using the Shrink API.\nKeep in mind that large shard sizes come with drawbacks, such as long full recovery times.\n4. Disable _source, The _source field stores the original JSON body of the document. If you don\u2019t need access to it you can disable it. However, APIs that needs access to _source such as update and reindex won\u2019t work.\n5. Use best_compression\n6. Force merge\n7. Shrink index, The shrink API allows you to reduce the number of shards in an index. Together with the force merge API above, this can significantly reduce the number of shards and segments of an index.\n8. Use the smallest numeric type that is sufficient\n9. Use index sorting to colocate similar documents\n10. Put fields in the same order in documents\n11. Roll up historical data\n12. Deleting old data or using index state management (ISM) to archive and delete data when it reaches a certain age.\n<url>\nhttps://www.elastic.co/guide/en/elasticsearch/reference/7.10/tune-for-disk-usage.html\n</url>   "}
{"question": "how to use ISM to delete old data?", "expectedAnswer": "To use ISM to delete old data, you need to create an index policy that defines the lifecycle phases and actions for your indices. \nFor example, you can create a policy that moves an index to the delete phase after a certain age and deletes it using the delete action. \nYou can also use other actions such as rollover, notification, snapshot, etc. to manage your indices before deleting them.\n<url>\nhttps://docs.aws.amazon.com/opensearch-service/latest/developerguide/ism.html\n</url> "}
{"question": "What is the difference between plugins.security.ssl.http.pemcert_filepath and plugins.security.ssl.transport.pemcert_filepath?", "expectedAnswer": "Both plugins.security.ssl.http.pemcert_filepath and plugins.security.ssl.transport.pemcert_filepath are configuration settings for SSL/TLS certificates in OpenSearch security plugins, but they apply to different components of the OpenSearch cluster.\nplugins.security.ssl.http.pemcert_filepath: This setting is used to specify the file path for the SSL/TLS certificate that should be presented by the OpenSearch HTTP server when clients connect to it over HTTPS. This certificate is used to authenticate the server to the client, and establish an encrypted connection.\nplugins.security.ssl.transport.pemcert_filepath: This setting is used to specify the file path for the SSL/TLS certificate that should be presented by the OpenSearch transport layer when nodes communicate with each other over the network. This certificate is used to authenticate nodes to each other, and establish a secure communication channel for cluster operations.\nIn summary, plugins.security.ssl.http.pemcert_filepath is used for the HTTPS server that clients use to connect to OpenSearch, while plugins.security.ssl.transport.pemcert_filepath is used for the SSL/TLS transport layer that connects OpenSearch nodes."}
{"question": "What is plugins.security.ssl.http.pemtrustedcas_filepath?", "expectedAnswer": "plugins.security.ssl.http.pemtrustedcas_filepath is a configuration setting in OpenSearch security plugin that specifies the file path for the SSL/TLS certificate authority (CA) that should be used to verify the authenticity of client certificates presented during HTTPS connections to the OpenSearch HTTP server.\nWhen a client connects to the OpenSearch HTTP server over HTTPS and presents its certificate, the server needs to verify that the certificate was issued by a trusted certificate authority. This is where the plugins.security.ssl.http.pemtrustedcas_filepath setting comes in.\nThis setting points to the file that contains the trusted CA certificates that the OpenSearch HTTP server should use to verify the authenticity of client certificates. If the client certificate presented during the HTTPS connection was issued by a trusted CA, the server will allow the connection to proceed. If the client certificate was not issued by a trusted CA, the server will reject the connection.\nIn summary, plugins.security.ssl.http.pemtrustedcas_filepath is used to specify the trusted CAs that the OpenSearch HTTP server should use to authenticate clients connecting over HTTPS."}
{"question": "How to set up OpenSearch and OpenSearch Dashboard v2.6 on a Linux EC2 instance.", "expectedAnswer": "To set up OpenSearch and OpenSearch Dashboard v2.6 on a Linux EC2 instance, you can follow the steps below:\nLaunch an EC2 instance: First, you need to launch an EC2 instance on Linux. You can choose any Linux AMI of your choice. Ensure that the instance has enough resources to run OpenSearch and OpenSearch Dashboard.\nInstall Java: OpenSearch requires Java to run, so you need to install Java on your EC2 instance. You can use the following command to install Java:\n<code>\nsudo yum install java-1.8.0-openjdk-devel\n</code>\nDownload and extract OpenSearch: Next, you need to download and extract the OpenSearch archive file. You can use the following commands to do that:\n<code>\nwget https://artifacts.opensearch.org/releases/bundle/opensearch/1.2.0/opensearch-1.2.0-linux-x64.tar.gz\ntar -xzvf opensearch-1.2.0-linux-x64.tar.gz\n</code>\nDownload and extract OpenSearch Dashboard: Similarly, you need to download and extract the OpenSearch Dashboard archive file. You can use the following commands to do that:<code>wget https://artifacts.opensearch.org/releases/bundle/opensearch-dashboard/1.2.0/opensearch-dashboard-1.2.0-linux-x64.tar.gz\ntar -xzvf opensearch-dashboard-1.2.0-linux-x64.tar.gz\n</code>"}
{"question": "What happened when OpenSearch returns errors like \u2018OpenSearch exception [type=illegal_argument_exception, reason=Text fields are not optimised for operations that require per-document field data like aggregations and sorting, so these operations are disabled by default. Please use a keyword field instead. Alternatively, set fielddata=true on [message] in order to load field data by uninverting the inverted index. Note that this can use significant memory\u2019 ?", "expectedAnswer": "When OpenSearch returns errors like what you\u2019ve put, that means you aggregate or sort from a field which is text data type, text fields fit for full text search, but not fit for aggregations, sorting or scripting, if you try to sort, aggregate, or access values from a text field, you will see an error indicating that field data is disabled by default by text fields. It\u2019s recommended to aggregate or sort from a keyword field, but you still want to use a text field, you should enable field data by setting `fieddata` to `true` for that field."}
{"question": "How can I set fielddata to true for one text field?", "expectedAnswer": "You can update the mapping of your index, set fielddata to true for the text field, like this:\nPUT test_index/_mapping\n{\n  \"properties\": {\n      \"message\":{\n        \"type\":\"text\",\n        \"fielddata\":true\n      }\n    }\n}"}
{"question": "Is there any bad effect if I set fielddata to true for the text field?", "expectedAnswer": "Yes, loading field data into memory can consume significant memory, you can configure the static setting `*indices.fielddata.cache.size*` to limit the memory size of field data, and by default the field data circuit breaker setting *indices.breaker.fielddata.limit *will limit the memory size of field data up to 40% of JVM heap, if loading the field will cause the field data cache to exceed the limit, the circuit breaker stops the request and returns an error."}
{"question": "What happened when I see the error `circuit_breaking_exception` in OpenSearch?", "expectedAnswer": "When you see `circuit_breaking_exception` in OpenSearch, that means the circuit breaker of OpenSearch was triggered and some operations might fail because of the memory limit. OpenSearch contains multiple circuit breakers to prevent operations from causing an OutOfMemoryError, for example,  during the process of a search request, OpenSearch may load field data into memory or generate huge global ordinals, and then cause the OpenSearch to crash because lack of memory. "}
{"question": "How can I set circuit breaker settings in my OpenSearch cluster?", "expectedAnswer": "There are many circuit breaker settings in OpenSearch, you can set them by using update cluster settings API, like this:\nPUT _cluster/settings\n{\n  \"persistent\": {\n      \"*indices.breaker.total.limit*\": \"70%\"\n    }\n}"}
{"question": "How can I resolve the issue when I see the circuit_breaking_exception in OpenSearch?", "expectedAnswer": "Firstly, you should identify which circuit breaker has been triggered, there are many circuit breakers, like parent level, field data, request, in-flight request, accounting, script compilation and regex, you can get the information from the log of client-side and server-side. Secondly, if you have identified which circuit breaker has been triggered, you can check the requests against OpenSearch and some metrics like JVM heap usage, node memory usage, field data cache size, request cache size etc. to get more information about why the circuit breaker is triggered and do some optimization,  for example, if you find that the field data cache is too large, then you can clear the cache to release some memory, and even optimize your query DSL to make sure that they will not consume significant memory. In another case, maybe the limit of circuit breaker is too low, then you can increase the limit by calling the update cluster settings API. If you\u2019ve done all of optimization work but nothing changes, you can consider adding more memory for the nodes or adding more nodes in your cluster."}
{"question": "I have all permissions for index-management plugin, but when I try to get a policy, I get this error:\n{\n\"error\": {\n\"root_cause\": [\n{\n\"type\": \"index_management_exception\",\n\"reason\": \"Do not have permission for policy [policy_2]\"\n}\n],\n\"type\": \"index_management_exception\",\n\"reason\": \"Do not have permission for policy [policy_2]\",\n\"caused_by\": {\n\"type\": \"exception\",\n\"reason\": \"org.opensearch.OpenSearchStatusException: Do not have permission for policy [policy_2]\"\n}\n},\n\"status\": 403\n}", "expectedAnswer": "The error message you are receiving indicates that you do not have permission to access the policy named \"policy_2\".\nEven if you have permissions for the index-management plugin, it's possible that your permissions do not include the ability to access or modify a specific policy.\nThe reason maybe that the cluster setting plugins.index_management.filter_by_backend_roles is set true. If so, there must be an intersection between the backend roles of the user and the backend roles of the resource.\nYou may need to check with the administrator of the OpenSearch instance to ensure that you have the appropriate permissions to access the policy you are trying to view."}
{"question": "give me the api to turn plugins.index_management.filter_by_backend_roles off", "expectedAnswer": "You can also use the persistent and transient parameters to set persistent and transient cluster settings respectively. Here's an example to change the setting plugins.index_management.filter_by_backend_roles:\n<code>\nPUT /_cluster/settings\n{\n\"transient\": {\n\"plugins.index_management.filter_by_backend_roles\": \"false\"\n}\n}\n</code>\n<code>\nPUT /_cluster/settings\n{\n\"persistent\": {\n\"plugins.index_management.filter_by_backend_roles\": \"false\"\n}\n}\n</code>"}
{"question": "There is a node in my cluster has performance issue. I call node stats api but all CPU/disk/heap usage is good. What reasons are most possible?", "expectedAnswer": "If the node stats API shows good CPU, disk, and heap usage, but you are still experiencing performance issues with the node, there could be several reasons for this:\n\n1. Network bandwidth: Your application may be experiencing performance issues due to network bandwidth limitations. Check if the node has enough network bandwidth to handle the traffic.\n\n2. IO throttling: If the application is doing heavy IO operations, the node might be throttling IO operations due to IO limitations. This can cause performance issues even if CPU/disk usage appears to be good.\n\n3. Other processes on the node: There may be other processes running on the node that are causing performance issues for your application. Check if there are any other processes running on the node that are competing for resources.\n\n4. Configuration issues: There may be configuration issues with the application or the node that are causing performance issues. Check if there are any configuration issues that need to be addressed.\n\nTo narrow down the root cause of the performance issue, you can use performance analyzer/root cause analysis plugin for troubleshooting."}
{"question": "How to get the network metric?", "expectedAnswer": "You can call performance analyzer metrics api. Metrics start with Net_ can be helpful for your question, for example, Net_PacketRate4, Net_PacketDropRate4.\nHere is an example to call this api:\n<code>\nGET _plugins/_performanceanalyzer/_agent/metrics?metrics=Net_PacketDropRate4&agg=max\n</code>"}
{"question": "how can i view audit log in opensearch?", "expectedAnswer": "To view the audit log in OpenSearch, you can follow these steps:\n1. Open the OpenSearch Management console in your web browser.\n2. Click on the \"Security\" tab in the left-hand menu.\n3. Click on the \"Audit logs\" link.\n4. Enable audit logging if it is not already enabled by clicking the toggle switch.\n5. Choose the type of audit logging you want to view from the drop-down menu. For example, you can choose \"Index events\" to view logs related to indexing operations.\n6. Create index pattern `security-auditlog*` for security audit logs,\n6. Use the Discover search and filter options to narrow down the log entries to the ones you're interested in.\n7. Click on individual log entries to view more details, or export the entire audit log to a file.\nNote that in order to view the audit log, you need to have the necessary permissions and access to the OpenSearch cluster. Additionally, audit logging may impact performance and resource usage, so it is recommended to use it judiciously and with caution."}
{"question": "which permisson is need to view audit log in opensearch?", "expectedAnswer": "To view the audit log in OpenSearch, you need to have the necessary permissions to access the OpenSearch cluster and read the audit log index.\nSpecifically, you need to have the read access to index `security-auditlog*`. \nYou can create a new role named audit_log_read_access and type `security-auditlog*` as index and select `read` as index permission. After that, map this role to the user you want to grant permission."}
{"question": "How can I setup alert on opensearch cluster/node stats data?", "expectedAnswer": "1 Click create monitor\n2 Choose Per cluster metrics monitor\n3 Select API you want to query on, the options are cluster health; node stats; cluster stats, etc\n4 Add a trigger and set trigger condition based on the response from the API you choose in step 3"}
{"question": "Although I set the frequency of alert to be every minute, I don\u2019t want to keep receiving the same alert if the problem exists for a while because it usually takes time to fix a problem, can I config the alert to only be triggered once during a fixed time interval? ", "expectedAnswer": "Yes, you can turn on Enable action throttling when you add the trigger to only trigger alert every x minutes."}
{"question": "I\u2019m an user having \u201call_access\u201d role, but I cannot delete index .opensearch-notifications-config \nDELETE .opensearch-notifications-config\n{ \"error\": { \"root_cause\": [ { \"type\": \"security_exception\", \"reason\": \"no permissions for [] and User [name=admin, backend_roles=[admin], requestedTenant=*user*]\" } ], \"type\": \"security_exception\", \"reason\": \"no permissions for [] and User [name=admin, backend_roles=[admin], requestedTenant=*user*]\" }, \"status\": 403 }", "expectedAnswer": "The error message suggests that the user \"admin\" with backend roles \"[admin]\" and requested tenant \"user\" does not have the necessary permissions to delete the \".opensearch-notifications-config\" index.\nThis index is an system index. System index can not be modified directly even users have \"all_access\" role."}
{"question": "I\u2019m an user having \u201call_access\u201d role, but I cannot delete index .opensearch-notifications-config \nDELETE .opensearch-notifications-config\n{ \"error\": { \"root_cause\": [ { \"type\": \"security_exception\", \"reason\": \"no permissions for [] and User [name=admin, backend_roles=[admin], requestedTenant=*user*]\" } ], \"type\": \"security_exception\", \"reason\": \"no permissions for [] and User [name=admin, backend_roles=[admin], requestedTenant=*user*]\" }, \"status\": 403 }. But I want to delete it. Is there any solution?", "expectedAnswer": "The final settings are not updateable through api. You can modify plugins.security.system_indices.enabled or plugins.security.system_indices.indices in config/opensearch.yml then restart the cluster.  "}
{"question": "I want to create a role to use index-state-management of index-management plugin, including index operations and send notificaitons. What permissions do I need to grant?", "expectedAnswer": "To use the Index State Management (ISM) feature of the Index Management plugin and perform index operations, as well as send notifications, you will need to give cluster permissions cluster:admin/opendistro/ism/* . IM plugins will perform index operations like shrink internally and you do not need to grant these permissions explicitly.\nHowever, to achieve your requirements, it is recommended to add permissions for cat indices and list notification channels.\nCat indices: indices: monitor/settings/get, indices:monitor/stats, cluster:monitor/state, cluster:monitor/health\nList notification channels: cluster:admin/opensearch/notifications/channels/get"}
{"question": "Now the user has required permission, but fail to send notifications(but I can). what\u2019s wrong?", "expectedAnswer": "If the policy and notification channel work fine, the reason may be that the setting opensearch.notifications.general.filter_by_backend_roles is turned on, which requires there is intersection between the backend roles of the user and the backend roles of the notification channel. "}
{"question": "what's the best practice for time serials data?", "expectedAnswer": "When it comes to working with time series data in OpenSearch, there are several best practices you can follow to ensure optimal performance and efficient use of resources:\n1. Use time-based indices: Organize your time series data into time-based indices, with each index representing a specific time interval (e.g., daily, hourly, or per minute). This can improve search performance and reduce resource usage by limiting the amount of data that needs to be queried and analyzed.\n2. Optimize shard size: Adjust the shard size of your time-based indices based on the amount of data being ingested and the query performance requirements. A good rule of thumb is to keep the shard size between 20-50GB to balance performance and resource usage.\n3. Use rollover indices: Configure index rollover to automatically create new indices when existing indices reach a certain size or age. This can help manage data retention policies and improve search performance by splitting large indices into smaller ones.\n4. Use time-based queries: Use time-based queries to filter data based on a specific time range or window. This can help limit the amount of data that needs to be searched and analyzed, which can improve query performance.\nBy following these best practices, you can optimize the performance and efficiency of your OpenSearch cluster when working with time series data."}
{"question": "how to create time-based index?", "expectedAnswer": "In OpenSearch, you can use date math expressions in index names to create dynamic index names based on the current date or other time-related variables. Here's an example of how you can use date math expressions in index names:\n<code>\n# PUT <my_index-{now/d{yyyy.MM.dd}}>\nPUT %3Cmy_index-%7Bnow%2Fd%7Byyyy.MM.dd%7D%7D%3E\n{\n    \"settings\": {\n    \"number_of_shards\": 1,\n    \"number_of_replicas\": 0\n    }\n}\n</code>\nIn this example, the {now/d{yyyy.MM.dd}} date math expression will calculate the current date in the specified format and insert it into the index name. The resulting index name will be something like \"my_index-2023.04.03\" if the current date is April 3, 2023."}
{"question": "Does OpenSearch support rollover time-based index?", "expectedAnswer": "No, OpenSearch don't support rollover time-based index. You can rollover a datastream or alias which point to a index and index name matches with pattern `'^.*-\\\\d+$'`. "}
{"question": "What happened when I see the error `es_rejected_execution_exception` with http status code 429 in OpenSearch?", "expectedAnswer": "When you see `es_rejected_execution_exception` with 429 in OpenSearch, that means the OpenSearch rejects some requests such as search requests or bulk requests because the related thread pool's queue is full. This may caused by a high load of the OpenSearch cluster or unbalanced shards between nodes in the OpenSearch cluster. To resolve this issue, you need to identify which thread pool is full firstly."}
{"question": "How can I identify which thread pool is full?", "expectedAnswer": "When you see the `es_rejected_execution_exception` with http status code 429 in OpenSearch, you can see the error in the running log of OpenSearch like `...name = xx/bulk, queue capacity = 200...`, then you know that the bulk thread pool's queue is full and the capacity of the queue is 200. If you cannot see the error in the running log of OpenSearch, you can use _cat/thread_pool API to fetch all of the thread pools with name, active threads, queued threads, and rejected threads, if one thread pool has rejected threads, that means this thread pool's queue was full."}
{"question": "How can I resolve the issue when I see the es_rejected_execution_exception in OpenSearch?", "expectedAnswer": "There are many reasons can cause the es_rejected_execution_exception, the typical cases are search rejection and bulk rejection.\nIf the search rejection is triggered, there maybe too much requests against your cluster and you can consider scaling up the node\u2019s spec in your cluster or adding more nodes to your cluster to support the query requests. you can also use _tasks API to check whether there are some long-running queries, if so you can consider canceling the long-running queries to remove them from the search thread pool\u2019s queue because they will block other incoming queries and consume the resource of system. \nIf the bulk rejection is triggered, you cluster may have low performance and cannot support too many write operations, you can also consider scaling up or scaling out you cluster. However, sometimes not all of the nodes in your cluster occurs bulk rejection, so you should identify which nodes occurred firstly and then dive deep. If only part of the nodes occur bulk rejection, the shards between each node may not be balanced, you can consider relocating some shards to balance the shards and system load between nodes. And more, you can check the shards\u2019 storage size, if there are some shards within the same index whose storage size is bigger than other shards obviously, you can relocate the shards to other nodes to balance the system load, but the most important thing is to find out why this happens, it maybe caused by the custom routing. Another reason about why only part of the nodes occur bulk rejection is that the index has too few shards to support the write operations, so you can rollover the index to a new index with proper number of primary shards in this situation."}
{"question": "I have done nothing, but why all of the indices in my OpenSearch cluster are blocked by read_only_allow_delete?", "expectedAnswer": "When all of the indices in you cluster are blocked by read_only_allow_delete, that means some nodes\u2019s disk usage exceed the value specified in the cluster setting *cluster.routing.allocation.disk.watermark.flood_stage,* when this happens OpenSearch will enforces a read-only-allow-delete block on every index that has one or more shards allocated on the nodes, this will protect the nodes from running out of disk space."}
{"question": "What can I do if I found all of the indices in my OpenSearch cluster were blocked by read_only_allow_delete?", "expectedAnswer": "In the short run, you can increase the value of the cluster setting *cluster.routing.allocation.disk.watermark.flood_stage *and remove the index block on every indices, and even you can disable the disk allocation decider by setting the cluster setting *cluster.routing.allocation.disk.threshold_enabled* to false.  In the long run, you can delete some unused indices to free the disk spaces,  add more nodes to your cluster, or scale up the disk size of the nodes. "}
{"question": "When I search against OpenSearch, if I use min_score parameter in the query DSL,  why sometimes the number of hits are different when I search more than one times?", "expectedAnswer": "When the `min_score` parameter is used in your query DSL, sometimes the number of documents that matching the query are different when you execute the query more than one times, that's a normal phenomenon because of the difference between the primary shard and the replicas. Write operations on primary shard and replica shard are independent, and the refresh operations are also independent, so primary shard and the replicas may have different segments. When executing the query DSL on different shards, the same document in primary shard and replicas may have different scores, if `min_score` is used to limit the score for matching documents, the query results maybe different. The same query maybe routed to primary shard or replica shards when it is executed more than one times, so the number of documents that matching the query maybe different."}
{"question": "How can I resolve this issue when I found that the number of hits are different when I execute the same query DSL more than one times?", "expectedAnswer": "You can use preference parameter to resolve this issue, this parameter can be any string does not start with _. When this parameter is used, search requests using the same value are routed to the same shards in the same order. \nAnother option is to use _forcemerge API to force merge the index to make each shard has only one segment so that the primary shard and the replica shards have no difference. But if the shard\u2019s storage size is too big like tens of GB, do not do this because it will cause poor search performance."}
{"question": "In anomaly detection plugin, will historical data analysis affect real-time detection job ?", "expectedAnswer": "Yes, the Anomaly Detection plugin use RRCF algorithm, which uses data points as parameters for inference. The historical analysis will affect the parameters of RRCF, and affect the inference for real-time detection job."}
{"question": "It seems that anomaly plugin can provide feature contribution for abnormal data points. I want to use this for casual analysis, only call detector api for abnormal data points.", "expectedAnswer": "This is not a good solution. If you only call detector api for abnormal data, the RRCF trees will be full of abnormal data, and \u201cabnormal\u201d will become \u201cnormal\u201d. Then the algorithm can not work as expected."}
{"question": "I have a product object with property of product title and product description, can you generate a effective query to fetch all the product related to \"Bread\"?", "expectedAnswer": "Sure! Assuming that you have an OpenSearch index containing your product data and that your product object has fields for product_title and product_description, you can use the following query to retrieve all products related to \"Bread\":\n<code>GET /your_index_name/_search { \"query\": { \"bool\": { \"should\": [ { \"match\": { \"product_title\": \"Bread\" } }, { \"match\": { \"product_description\": \"Bread\" } } ] } } }</code>\nThis query uses a bool query with two should clauses, which means that any product that matches either of the match queries will be returned. The first match query looks for the term \"Bread\" in the product_title field, while the second match query looks for the term \"Bread\" in the product_description field.\nYou can customize this query by adjusting the field names or by using different query types depending on your specific needs."}
{"question": "What are regular alerts I can set up to Monitor a OpenSearch Cluster?", "expectedAnswer": "Monitoring your OpenSearch cluster is an important aspect of maintaining its health and performance. Here are some regular alerts that you can set up to monitor your OpenSearch cluster:\nHigh CPU usage: You can set up an alert to trigger when the CPU usage on any node in your cluster exceeds a certain threshold. High CPU usage can indicate that a node is under stress, and it can cause performance issues.\nHigh heap usage: You can set up an alert to trigger when the heap usage on any node in your cluster exceeds a certain threshold. High heap usage can lead to garbage collection issues, which can cause performance problems.\nDisk usage: You can set up an alert to trigger when the disk usage on any node in your cluster exceeds a certain threshold. High disk usage can cause performance issues and can also indicate that you need to add more storage to your cluster.\nMemory pressure: You can set up an alert to trigger when the memory pressure on any node in your cluster exceeds a certain threshold. Memory pressure can indicate that a node is under stress and can lead to performance issues.\nQuery latency: You can set up an alert to trigger when the query latency on any node in your cluster exceeds a certain threshold. High query latency can indicate that there is a problem with your cluster's performance, and it can also impact user experience.\nNode failures: You can set up an alert to trigger when any node in your cluster fails or goes offline. Node failures can impact the availability of your cluster and can cause data loss if not addressed promptly.\nThese are just a few examples of the types of alerts you can set up to monitor your OpenSearch cluster. You can customize your alerts based on your specific needs and environment."}
{"question": "My cluster is runing out of disk, show me the steps of add a new data node", "expectedAnswer": "To add a new data node to your cluster, you can follow these steps\n1. Set up a new OpenSearch instance. Specify the name of the cluster with the cluster.name setting in opensearch.yml. \nFor example, to add a node to the logging-prod cluster, add the line cluster.name: \u201clogging-prod\u201d to opensearch.yml.\n2. Configure the new node: Configure the new node to join the existing cluster by editing the opensearch.yml configuration file with settings `discovery.seed_hosts` to include the IP address or hostname of at least one existing data node in the cluster.\n\n3. Start OpenSearch. The node automatically discovers and joins the specified cluster.\n\n4. Verify that the new node has joined the cluster: Use the cluster health API or the cluster state API to verify that the new node has successfully joined the cluster and is visible to the other nodes.\n5 .Configure shard allocation: After the new node is added, you may want to adjust the shard allocation settings to ensure that new shards are assigned to the new node to balance the disk usage across the cluster. You can use the shard allocation filtering API to exclude the new node from receiving shards until it has enough disk space to handle them.\n6. Monitor the cluster: After adding the new data node, monitor the cluster's health and performance to ensure that the new node is functioning correctly and is not causing any issues."}
{"question": "show me an example of how to set `discovery.seed_hosts` in opensearch.yml", "expectedAnswer": "The discovery.seed_hosts setting can be specified in the opensearch.yml configuration file on each node, and should contain a comma-separated list of IP addresses or hostnames of seed nodes. \n<code>\ndiscovery.seed_hosts: [\"192.168.1.10\", \"192.168.1.11\", \"192.168.1.12\"]\n</code>\nNote that the `discovery.seed_hosts` setting should contain at least one valid seed node. Otherwise, the node will not be able to discover any other nodes in the cluster and will not be able to join the cluster."}
{"question": "how to find the node with highest disk usage?", "expectedAnswer": "You can use below command to get the highest disk usage node\n<code>\nGET _cat/nodes?v&h=name,disk.used&bytes=b&s=disk.used:desc\n</code>\nThis will return a table with two columns: name (the name of the node) and disk.used (the amount of disk space used by the node in bytes). The first row and first column is the node name."}
{"question": "my cluster has high cpu usage, i want to know which tasks are running now", "expectedAnswer": "To list all task running now, you can use below command \n<code>\nGET _cat/tasks\n</code>\n, it will list all tasks running by now, you can check the running_time column to find out the task with longest time and check task detail with \n<code>\nGET _taak/taskId\n</code>.\nOnce you have identified the tasks that are consuming CPU resources, you can investigate further to determine the root cause of the high CPU usage. For example, you may need to optimize your queries, increase the number of nodes in your cluster, or adjust your resource allocation settings to better balance the workload."}
{"question": "I have identified that a bad query is consuming CPU resource, can i cancel it?", "expectedAnswer": "Yes. you can cancel it. First, you need to get the exact task id for the bad query, and then use task cancellation API to cancel it. See below example: \n<code>\nPOST /_tasks/{task_id}/_cancel\n</code>\nOnce you have canceled the query, the task will be terminated and any resources that were being consumed by the query will be freed up"}
{"question": "how to optimize on query return large result?", "expectedAnswer": "Returning a large number of results can impact query performance and consume significant network and memory resources. Here are some tips to optimize querying for large result sets:\n1. Use pagination: To avoid returning a large number of results in a single query, use pagination techniques such as limit/offset, scroll API, or search after.\n2. Use request compression: Compressing requests can help reduce the network load when transferring large amounts of data. OpenSearch supports request compression using Gzip and Deflate.\n3.Use source filtering: Source filtering allows you to exclude or include specific fields in the returned documents. By only returning the required fields, you can reduce the size of the response payload."}
{"question": "how to use search after to do pageniation", "expectedAnswer": "Search After is a pagination method in OpenSearch that allows you to retrieve a large number of results by iterating over them in multiple requests. Here's how you can use the search after API to implement pagination:\n\n1. Execute the initial search request with a limit of the number of results you want to retrieve in a single page, and a sorting order that defines how the results should be ordered. For example:\n<code>\nGET /myindex/_search\n{\n    \"size\": 10,\n    \"sort\": [\n        {\"timestamp\": \"desc\"},\n        {\"_id\": \"desc\"}\n    ],\n    \"query\": {\n        \"match_all\": {}\n    }\n}\n</code>\n2. After receiving the initial response, extract the value of the last sort field for the last document in the result set. In this example, it would be the _id value of the last document.\n3. In the next request, use the search after parameter to specify the value of the last sort field from the previous response. For example:\n<code>\nGET /myindex/_search\n{\n    \"size\": 10,\n    \"sort\": [\n        {\"timestamp\": \"desc\"},\n        {\"_id\": \"desc\"}\n    ],\n    \"query\": {\n        \"match_all\": {}\n    },\n    \"search_after\": [ last_timestamp, last_id ]\n}\n</code>\n4. Repeat steps 2-3 for each subsequent request until you have retrieved all the documents you need.\nNote that search after pagination is based on the concept of a cursor, where the cursor points to the last record that was retrieved. It works best for scenarios where you have a unique sort field such as _id, and the total number of documents to retrieve is unknown. Also, be aware that the cursor may become invalid if the underlying data changes during pagination."}
{"question": "As underlying data changes may cause search_after return unexpected result, how to achieve search_after with consistent data?", "expectedAnswer": "To achieve search_after with consistent data, you can use a combination of two techniques:\n1. Point in Time (PIT) API: The PIT API allows you to take a snapshot of the state of an index at a particular point in time, and perform a search operation using that snapshot. This guarantees that the data returned by the search operation is consistent.\nCreate a PIT first\n<code>\nPOST /my_index/_pit\n</code>\n, that will return a PIT id \n<code>\n{\"id\": \"v0F2-2gBbTR1a2xvMVNCTA==\"}\n</code>\n\n2. Use search_after mentioned above\n<code>\nGET /myindex/_search\n{\n    \"size\": 10000,\n    \"query\": {\n        \"match\" : {\n        \"user.id\" : \"elkbee\"\n        }\n    },\n    \"pit\": {\n        \"id\":  \"v0F2-2gBbTR1a2xvMVNCTA==\", \n        \"keep_alive\": \"100m\"\n    },\n    \"sort\": [ \n        {\"timestamp\": \"desc\"},\n        {\"_id\": \"desc\"}\n    ]\n}\n</code>"}
{"question": "Why do I get ClusterBlockException in OpenSearch?", "expectedAnswer": "There are a few common reasons for a cluster block:\n1.Read-only index block: This occurs when an index is in read-only mode, which may be due to a full disk or a snapshot restore operation. In this state, all write requests to the index are blocked.\n2.Lack of free storage space.\n3.High JVM memory pressure: the JVMMemoryPressure metric exceeds 92% for 30 minutes"}
{"question": "Why I get high memory pressure?", "expectedAnswer": "1. Too many shards on the node\n2. Expense query that has aggregating on text fields, etc."}
{"question": "How to solve high memory pressure problem?", "expectedAnswer": "1. Clear the data cache with the POST index-name/_cache/clear?fielddata=true API operation. Note that clearing the cache can disrupt in-progress queries.2. Expense query that has aggregating on text fields, etc.\n2. Reduce number of shards on a node."}
